"""
Ko-LLaVA ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ChatVectorë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë°ëª¨
"""

from src import ChatVectorMerger, KoLLaVAPredictor, EnvironmentConfig, setup_logger

# ë¡œê±° ì„¤ì • (ë¡œê·¸ íŒŒì¼ë„ í•¨ê»˜ ê¸°ë¡)
logger = setup_logger("ko-llava-main", level="INFO", log_file="logs/ko-llava.log")

def run_merging_pipeline():
    """ëª¨ë¸ ë³‘í•© íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger.info("ğŸš€ Starting ChatVector merging pipeline...")
    
    merger = ChatVectorMerger()
    try:
        merger.run_full_pipeline(alpha=1.0)
        logger.info("âœ… Merging pipeline completed successfully!")
    except Exception as e:
        logger.error(f"âŒ Merging pipeline failed: {e}")
        raise
    finally:
        merger.cleanup_models()

def run_inference_demo():
    """ì¶”ë¡  ë°ëª¨ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger.info("ğŸ”„ Starting inference demo...")
    
    predictor = KoLLaVAPredictor()
    try:
        # ëª¨ë¸ ë¡œë“œ
        predictor.load_model()
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            {
                "name": "ì´ë¯¸ì§€ ì„¤ëª…",
                "prompt": predictor.create_prompt("ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
                "image_url": "https://cdn-uploads.huggingface.co/production/uploads/5e56829137cb5b49818287ea/NWfoArWI4UPAxpEnolkwT.jpeg"
            },
            {
                "name": "í•œêµ­ ì¥ì†Œ ì‹ë³„", 
                "prompt": predictor.create_prompt("ì´ê³³ì€ ëŒ€í•œë¯¼êµ­ì˜ ì–´ë””ì¸ê°€ìš”?"),
                "image_url": "https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyNDAzMzFfNDYg%2FMDAxNzExODMzNTM4MTU5.Xf8te7rReNi4aXtFAsjjdeCsXDv1Tr4Be5pOsuofd0Mg.i8UclMMaD91i0MEMEXXKsgloQKZQbJfVJQeqK_2UC8Yg.PNG%2F359d2185%25A3%25ADc597%25A3%25AD49a3%25A3%25ADb102%25A3%25ADdf25158be59f.png&type=sc960_832"
            }
        ]
        
        # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"\n=== í…ŒìŠ¤íŠ¸ {i}: {test_case['name']} ===")
            
            try:
                response = predictor.predict(
                    prompt=test_case["prompt"],
                    image_url=test_case["image_url"],
                    max_new_tokens=512,
                    temperature=0.7
                )
                logger.info(f"âœ¨ ì‘ë‹µ:\n{response}\n")
                
            except Exception as e:
                logger.error(f"âŒ í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {e}")
                
    except Exception as e:
        logger.error(f"âŒ Inference demo failed: {e}")
        raise
    finally:
        predictor.cleanup()

def run_interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger.info("ğŸ® Starting interactive mode...")
    
    predictor = KoLLaVAPredictor()
    predictor.load_model()
    
    print("\n" + "="*50)
    print("ğŸ‡°ğŸ‡· Ko-LLaVA ëŒ€í™”í˜• ëª¨ë“œ")
    print("ì´ë¯¸ì§€ URLê³¼ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì—¬ í•œêµ­ì–´ ì‘ë‹µì„ ë°›ì•„ë³´ì„¸ìš”!")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥")
    print("="*50 + "\n")
    
    try:
        while True:
            try:
                user_input = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    break
                
                if not user_input:
                    print("âš ï¸  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                image_url = input("ğŸ–¼ï¸  ì´ë¯¸ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if not image_url:
                    print("âš ï¸  ì´ë¯¸ì§€ URLì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    continue
                
                print("\nğŸ¤– ì‘ë‹µ ìƒì„± ì¤‘...")
                prompt = predictor.create_prompt(user_input)
                response = predictor.predict(prompt, image_url, stream=True)
                print(f"\nâœ¨ ì‘ë‹µ:\n{response}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
    finally:
        predictor.cleanup()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ‡°ğŸ‡· Ko-LLaVA ì‹œì‘!")
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\n" + "="*50)
    print("ğŸ‡°ğŸ‡· Ko-LLaVA: ChatVector ê¸°ë°˜ í•œêµ­ì–´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸")
    print("="*50)
    print("1. ëª¨ë¸ ë³‘í•© (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)")
    print("2. ì¶”ë¡  ë°ëª¨ ì‹¤í–‰")
    print("3. ëŒ€í™”í˜• ëª¨ë“œ")
    print("="*50)
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
    
    try:
        if choice == "1":
            run_merging_pipeline()
        elif choice == "2":
            run_inference_demo()
        elif choice == "3":
            run_interactive_mode()
        else:
            logger.info("â„¹ï¸  ì¶”ë¡  ë°ëª¨ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            run_inference_demo()
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    logger.info("ğŸ‘‹ Ko-LLaVA ì¢…ë£Œ!")


if __name__ == "__main__":
    main()
