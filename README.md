## Llama3-Chat_Vector-kor_llava : Llama3 ê¸°ë°˜ í•œêµ­ì–´ LLAVA ëª¨ë¸
---

### Update Logs
- 2024.06.27: [ğŸ¤—Llama3 ê¸°ë°˜ í•œêµ­ì–´ LLAVA ëª¨ë¸ ê³µê°œ](nebchi/Llama3-Chat_Vector-kor_llava)

### Reference Models:
1) beomi/Llama-3-KoEn-8B(https://huggingface.co/beomi/Llama-3-KoEn-8B)
2) xtuner/llava-llama-3-8b-transformers(https://huggingface.co/xtuner/llava-llama-3-8b-transformers)
3) Chat-Vector(https://arxiv.org/abs/2310.04799)

**Model Developers**: nebchi

## Model Description
* ì´ë²ˆ Kor-LLAVA ëª¨ë¸ì€ ëŒ€ëŸ‰ì˜ í•œêµ­ì–´ ì½”í¼ìŠ¤ë¡œ ì‚¬ì „í•™ìŠµí•œ LLAMA ëª¨ë¸ê³¼ LLAVA ëª¨ë¸ì„ Chat_Vectorë¥¼ í™œìš©í•˜ì—¬, ì´ë¯¸ì§€ì— ëŒ€í•˜ì—¬, í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ì´ ê°€ëŠ¥í•œ VLM ëª¨ë¸ ì…ë‹ˆë‹¤.

<p align="left" width="100%">
<img src="assert/Seoul_city.png" alt="NLP Logo" style="width: 40%;">
</p>

### Running the model on GPU
```python
import requests
from PIL import Image

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, TextStreamer

model_id = "nebchi/Llama3-Chat_Vector-kor_llava"

model = LlavaForConditionalGeneration.from_pretrained(
    model_id, 
    torch_dtype='auto', 
    device_map='auto',
    revision='a38aac3', 
)

processor = AutoProcessor.from_pretrained(model_id)

tokenizer = processor.tokenizer
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
streamer = TextStreamer(tokenizer)

prompt = ("<|start_header_id|>user<|end_header_id|>\n\n<image>\nì´ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.<|eot_id|>"
          "<|start_header_id|>assistant<|end_header_id|>\n\nì´ ì´ë¯¸ì§€ì—ëŠ”")
image_file = "https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyNDAzMzFfNDYg%2FMDAxNzExODMzNTM4MTU5.Xf8te7rReNi4aXtFAsjjdeCsXDv1Tr4Be5pOsuofd0Mg.i8UclMMaD91i0MEMEXXKsgloQKZQbJfVJQeqK_2UC8Yg.PNG%2F359d2185%25A3%25ADc597%25A3%25AD49a3%25A3%25ADb102%25A3%25ADdf25158be59f.png&type=sc960_832"

raw_image = Image.open(requests.get(image_file, stream=True).raw)
inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)

output = model.generate(
    **inputs,
    max_new_tokens=512,
    do_sample=True,  
    eos_token_id=terminators,
    no_repeat_ngram_size=3, 
    temperature=0.7,  
    top_p=0.9,  
    streamer=streamer
)
print(processor.decode(output[0][2:], skip_special_tokens=False))
```

### results
```python
ì´ ì´ë¯¸ì§€ì—ëŠ” ë„ì‹œì˜ ëª¨ìŠµì´ ì˜ ë³´ì—¬ì§‘ë‹ˆë‹¤. ë„ì‹œ ë‚´ë¶€ì—ëŠ” ì—¬ëŸ¬ ê±´ë¬¼ê³¼ ê±´ë¬¼ë“¤ì´ ìˆê³ , ë„ì‹œë¥¼ ì—°ê²°í•˜ëŠ” ë„ë¡œì™€ êµí†µ ì‹œìŠ¤í…œì´ ì˜ ë°œë‹¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ë„ì‹œì˜ íŠ¹ì§•ì€ ë†’ê³  ê´‘ë²”ìœ„í•œ ê±´ë¬¼ë“¤ê³¼ êµí†µë§ì„ ê°–ì¶˜ ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
```
---
**Citation**

```bibtex
@misc {Llama3-Chat_Vector-kor_llava,
	author       = { {nebchi} },
	title        = { Llama3-Chat_Vector-kor_llava },
	year         = 2024,
	url          = { https://huggingface.co/nebchi/Llama3-Chat_Vector-kor_llava },
	publisher    = { Hugging Face }
}
