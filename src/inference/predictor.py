"""
Ko-LLaVA ì¶”ë¡  ëª¨ë“ˆ

ë³‘í•©ëœ í•œêµ­ì–´ LLaVA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, TextStreamer
from PIL import Image
import requests
from typing import Optional
from pathlib import Path

from src.config.settings import EnvironmentConfig
from src.utils.logger import setup_logger

env = EnvironmentConfig.load()

class KoLLaVAPredictor:
    """í•œêµ­ì–´ LLaVA ëª¨ë¸ ì¶”ë¡  í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 model_path: str = env.OUTPUT_DIR,
                 base_model_name: str = env.LLAMA_MODEL_NAME):
        """
        ì¶”ë¡ ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: ë³‘í•©ëœ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ
            base_model_name: í”„ë¡œì„¸ì„œ ë¡œë“œë¥¼ ìœ„í•œ ë² ì´ìŠ¤ ëª¨ë¸ëª…
        """
        self.model_path = model_path
        self.base_model_name = base_model_name
        
        # ë¡œê±° ì„¤ì •
        self.logger = setup_logger("ko-llava-predictor", level="INFO")
        
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.streamer = None
    
    def load_model(self, revision: str = 'main'):
        """ë³‘í•©ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.logger.info(f"ğŸ”„ Loading merged Ko-LLaVA model from {self.model_path}...")
        
        self.model = LlavaForConditionalGeneration.from_pretrained(
            self.model_path,
            torch_dtype='auto',
            device_map='auto',
            revision=revision
        )
        
        # í”„ë¡œì„¸ì„œì™€ í† í¬ë‚˜ì´ì € ì„¤ì •
        self.processor = AutoProcessor.from_pretrained(self.base_model_name)
        self.tokenizer = self.processor.tokenizer
        
        # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì •
        self.streamer = TextStreamer(self.tokenizer)
        
        self.logger.info("âœ… Ko-LLaVA model loaded successfully!")
    
    def predict(self, 
                prompt: str, 
                image_url: str,
                max_new_tokens: int = 512,
                temperature: float = 0.7,
                top_p: float = 0.9,
                no_repeat_ngram_size: int = 3,
                stream: bool = False) -> str:
        """
        ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            prompt: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            image_url: ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„± ì¡°ì ˆ
            top_p: top-p ìƒ˜í”Œë§ ê°’
            no_repeat_ngram_size: n-gram ë°˜ë³µ ë°©ì§€
            stream: ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        if self.model is None:
            raise ValueError("Model not loaded. Call load_model() first.")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        if image_url.startswith(('http://', 'https://')):
            raw_image = Image.open(requests.get(image_url, stream=True).raw)
        else:
            raw_image = Image.open(image_url)
        
        # ì…ë ¥ ì „ì²˜ë¦¬
        inputs = self.processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)
        
        # ì¢…ë£Œ í† í° ì„¤ì •
        terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        
        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                eos_token_id=terminators,
                no_repeat_ngram_size=no_repeat_ngram_size,
                temperature=temperature,
                top_p=top_p,
                streamer=self.streamer if stream else None
            )
        
        # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
        response = self.processor.decode(output[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)
        return response.strip()
    
    def create_prompt(self, user_message: str, assistant_start: str = "ì´ ì´ë¯¸ì§€ì—ëŠ”") -> str:
        """í‘œì¤€ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        return (f"<|start_header_id|>user<|end_header_id|>\n\n<image>\n{user_message}<|eot_id|>"
                f"<|start_header_id|>assistant<|end_header_id|>\n\n{assistant_start}")
    
    def batch_predict(self, prompts_and_images: list, **generate_kwargs) -> list:
        """
        ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•´ ë°°ì¹˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            prompts_and_images: [(prompt, image_url), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
            **generate_kwargs: ìƒì„± íŒŒë¼ë¯¸í„°ë“¤
            
        Returns:
            ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for prompt, image_url in prompts_and_images:
            try:
                response = self.predict(prompt, image_url, **generate_kwargs)
                results.append(response)
            except Exception as e:
                self.logger.error(f"âŒ Error processing {image_url}: {e}")
                results.append(f"Error: {str(e)}")
        return results
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ëª¨ë¸ì„ í•´ì œí•©ë‹ˆë‹¤."""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        torch.cuda.empty_cache()
        self.logger.info("ğŸ§¹ Predictor cleaned up from memory!")
