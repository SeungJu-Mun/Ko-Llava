"""
ChatVector êµ¬í˜„ ëª¨ë“ˆ

ChatVector ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•˜ì—¬ 
í•œêµ­ì–´ Llama ëª¨ë¸ì˜ ì§€ì‹ì„ LLaVA ëª¨ë¸ì— ì „ì´í•©ë‹ˆë‹¤.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import requests
from typing import List, Optional
from pathlib import Path

from src.config.settings import EnvironmentConfig
from src.utils.logger import setup_logger

env = EnvironmentConfig.load() 

class ChatVectorMerger:
    """
    ChatVectorë¥¼ ì´ìš©í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ê³¼ í•œêµ­ì–´ ëª¨ë¸ ê²°í•© í´ë˜ìŠ¤
    """
    
    def __init__(self, 
                 base_model_name: str = env.LLAMA_MODEL_NAME,
                 ko_model_name: str = env.LLAVA_MODEL_NAME,
                 cache_dir: str = env.CACHE_DIR,
                 output_dir: str = env.OUTPUT_DIR):
        """
        ChatVector ì´ˆê¸°í™”
        
        Args:
            base_model_name: ë² ì´ìŠ¤ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ëª…
            ko_model_name: í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ ëª¨ë¸ëª…
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            output_dir: ê²°í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.base_model_name = base_model_name
        self.ko_model_name = ko_model_name
        self.cache_dir = cache_dir
        self.output_dir = output_dir
        
        # ë¡œê±° ì„¤ì •
        self.logger = setup_logger("chat-vector", level="INFO")
        
        # ì œì™¸í•  ë ˆì´ì–´ ì •ì˜
        self.skip_layers = [
            "model.embed_tokens.weight",
            "lm_head.weight", 
            "vision_tower.vision_model.embeddings.class_embedding",
            "vision_tower.vision_model.embeddings.patch_embedding.weight",
            "vision_tower.vision_model.embeddings.position_embedding.weight",
            "vision_tower.vision_model.pre_layrnorm.weight"
        ]
        
        self.base_model = None
        self.ko_model = None
        self.merged_model = None
    
    def load_models(self):
        """ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í•œêµ­ì–´ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        self.logger.info("ğŸ”„ Loading base multimodal model...")
        self.base_model = LlavaForConditionalGeneration.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            cache_dir=self.cache_dir
        )
        
        self.logger.info("ğŸ”„ Loading Korean pretrained model...")
        self.ko_model = AutoModelForCausalLM.from_pretrained(
            self.ko_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto", 
            cache_dir=self.cache_dir
        )
        
        self.logger.info("âœ… Models loaded successfully!")
    
    def merge_models(self, alpha: float = 1.0):
        """
        ChatVectorë¥¼ ì´ìš©í•´ ëª¨ë¸ë“¤ì„ ê²°í•©í•©ë‹ˆë‹¤.
        
        Args:
            alpha: ê²°í•© ê°•ë„ ì¡°ì ˆ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 1.0)
        """
        if self.base_model is None or self.ko_model is None:
            raise ValueError("Models must be loaded first. Call load_models() first.")
        
        self.logger.info(f"ğŸ”„ Merging models using ChatVector (alpha={alpha})...")
        
        # ìƒˆë¡œìš´ ëª¨ë¸ ìƒíƒœ ì‚¬ì „ ìƒì„±
        new_state_dict = self.base_model.state_dict()
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        for k, v in self.ko_model.state_dict().items():
            # ìŠ¤í‚µí•  ë ˆì´ì–´ë‚˜ layernorm ë ˆì´ì–´ëŠ” ì œì™¸
            if any(skip in k for skip in self.skip_layers) or ("layernorm" in k):
                continue
                
            if k in new_state_dict:
                # ChatVector ê³„ì‚°: base_model[k] - ko_model[k]  
                chat_vector = self.base_model.state_dict()[k] - self.ko_model.state_dict().get(k, torch.zeros_like(v))
                # ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜: ko_model[k] + alpha * chat_vector
                new_v = v + alpha * chat_vector.to(v.device)
                # ë² ì´ìŠ¤ ëª¨ë¸ì— ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ì ìš©
                with torch.no_grad():
                    new_state_dict[k].copy_(new_v)
        
        self.logger.info("âœ… Model merging completed!")
    
    def save_merged_model(self):
        """ê²°í•©ëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
        if self.base_model is None:
            raise ValueError("No model to save. Merge models first.")
        
        self.logger.info(f"ğŸ’¾ Saving merged model to {self.output_dir}...")
        self.base_model.save_pretrained(self.output_dir)
        self.logger.info("âœ… Model saved successfully!")
    
    def run_full_pipeline(self, alpha: float = 1.0):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ (ëª¨ë¸ ë¡œë“œ -> ê²°í•© -> ì €ì¥)"""
        self.logger.info("ğŸš€ Starting ChatVector full pipeline...")
        self.load_models()
        self.merge_models(alpha=alpha)
        self.save_merged_model()
        self.logger.info("ğŸ‰ ChatVector pipeline completed!")
        
    def cleanup_models(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ëª¨ë¸ë“¤ì„ í•´ì œí•©ë‹ˆë‹¤."""
        if self.base_model is not None:
            del self.base_model
            self.base_model = None
        if self.ko_model is not None:
            del self.ko_model  
            self.ko_model = None
        if self.merged_model is not None:
            del self.merged_model
            self.merged_model = None
        torch.cuda.empty_cache()
        self.logger.info("ğŸ§¹ Models cleaned up from memory!")